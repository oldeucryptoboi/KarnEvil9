<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Memory-Assisted Learning vs Recursive Language Models — An Empirical Study</title>

  <!-- Authorship -->
  <meta name="author" content="Laurent DeSegur">

  <!-- SEO -->
  <meta name="description" content="An empirical comparison of Memory-Assisted Learning (MAL) and prompt-recursive lesson injection for enabling LLMs to improve at strategic tasks without weight updates. 72-game tic-tac-toe experiment using a 3-node swarm mesh demonstrates that hybrid memory+code architectures eliminate losses while pure prompt-recursive learning hits a reliability ceiling.">
  <meta name="keywords" content="LLM learning, recursive language modeling, memory-assisted learning, agentic AI, self-improving AI, tic-tac-toe AI, knowing-doing gap, Claude, Anthropic, swarm agents, multi-agent systems, in-context learning, AI memory systems, KarnEvil9, prompt engineering, AI architecture, token economics, LLM cost optimization, context window scaling, Reflexion, prompt-recursive learning">
  <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large">
  <link rel="canonical" href="https://github.com/oldeucryptoboi/KarnEvil9/blob/master/docs/memory-vs-rlm.html">

  <!-- Open Graph / Social -->
  <meta property="og:type" content="article">
  <meta property="og:title" content="Memory-Assisted Learning vs Recursive Language Models">
  <meta property="og:description" content="Can an LLM improve at a game by reading its own past failures? An empirical comparison using a swarm-based tic-tac-toe testbed shows hybrid memory+code beats pure prompt-recursive self-improvement.">
  <meta property="og:site_name" content="KarnEvil9 Project">
  <meta property="article:author" content="Laurent DeSegur">
  <meta property="article:published_time" content="2026-02-09">
  <meta property="article:section" content="AI Research">
  <meta property="article:tag" content="LLM">
  <meta property="article:tag" content="Recursive Language Modeling">
  <meta property="article:tag" content="Memory-Assisted Learning">
  <meta property="article:tag" content="Agentic AI">
  <meta property="article:tag" content="Multi-Agent Systems">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Memory-Assisted Learning vs Recursive Language Models">
  <meta name="twitter:description" content="Can an LLM improve at a game by reading its own past failures? 72-game empirical study comparing two learning paradigms for LLMs without weight updates.">

  <!-- Generative Engine Optimization (GEO) -->
  <meta name="citation_title" content="Memory-Assisted Learning vs Recursive Language Models: An Empirical Study">
  <meta name="citation_author" content="Laurent DeSegur">
  <meta name="citation_date" content="2026-02-09">
  <meta name="citation_journal_title" content="KarnEvil9 Technical Reports">

  <!-- Schema.org structured data for GEO -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "TechArticle",
    "headline": "Memory-Assisted Learning vs Recursive Language Models: An Empirical Study",
    "alternativeHeadline": "Can an LLM improve at a game by reading its own past failures?",
    "author": {
      "@type": "Person",
      "name": "Laurent DeSegur"
    },
    "datePublished": "2026-02-09",
    "description": "An empirical comparison of Memory-Assisted Learning (MAL) and prompt-recursive lesson injection for enabling LLMs to improve at strategic tasks without weight updates. The LLM plays first (O) against a deterministic minimax opponent (X) with a 10% random blunder rate. Over 72 games, MAL learns to reliably capitalize on those blunders, achieving zero losses by run 3, while prompt-recursive learning reduces losses but cannot eliminate them, revealing a fundamental knowing-doing gap in LLM reasoning.",
    "keywords": ["LLM learning", "recursive language modeling", "memory-assisted learning", "agentic AI", "self-improving AI", "knowing-doing gap", "multi-agent systems", "in-context learning", "token economics", "LLM cost optimization", "prompt-recursive learning"],
    "about": [
      {"@type": "Thing", "name": "Large Language Models"},
      {"@type": "Thing", "name": "Machine Learning"},
      {"@type": "Thing", "name": "Artificial Intelligence"},
      {"@type": "Thing", "name": "Multi-Agent Systems"}
    ],
    "isPartOf": {
      "@type": "SoftwareSourceCode",
      "name": "KarnEvil9",
      "codeRepository": "https://github.com/oldeucryptoboi/KarnEvil9"
    },
    "mainEntityOfPage": {
      "@type": "WebPage"
    },
    "proficiencyLevel": "Expert",
    "dependencies": "Claude Haiku 4.5, Claude Sonnet 4.5, KarnEvil9 Swarm Package",
    "abstract": "We compare two approaches for enabling a large language model to improve at a strategic game without weight updates: Memory-Assisted Learning (MAL), which augments an LLM with persistent external memory and programmatic rules derived from accumulated lessons, and prompt-recursive lesson injection, where the model's own prior analytical outputs feed back into its prompt context. The LLM plays first (O) against a deterministic minimax opponent (X) with a 10% random blunder rate. Over 72 games, MAL achieves zero losses by run 3, while prompt-recursive learning reduces losses but cannot eliminate them, revealing a fundamental knowing-doing gap."
  }
  </script>

  <style>
    :root{
      --bg:#ffffff;
      --fg:#111827;
      --muted:#4b5563;
      --border:#e5e7eb;
      --codebg:#0b1020;
      --codefg:#e5e7eb;
      --accent:#2563eb;
      --callout:#f3f4f6;
      --warn:#fff7ed;
      --warnborder:#fed7aa;
      --good:#ecfdf5;
      --goodborder:#a7f3d0;
      --bad:#fef2f2;
      --badborder:#fecaca;
    }
    html,body{background:var(--bg); color:var(--fg); font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji","Segoe UI Emoji"; line-height:1.55; margin:0;}
    main{max-width: 980px; margin: 0 auto; padding: 28px 18px 80px;}
    h1,h2,h3{line-height:1.25; margin: 1.2em 0 0.5em;}
    h1{font-size: 2.05rem; margin-top: 0.2em;}
    h2{font-size: 1.55rem; padding-top: 0.6em; border-top: 1px solid var(--border);}
    h3{font-size: 1.18rem;}
    p{margin: 0.6em 0;}
    a{color:var(--accent); text-decoration: none;}
    a:hover{text-decoration: underline;}
    .meta{color:var(--muted); margin-top: 6px;}
    .author{font-size: 1.1rem; font-weight: 500; margin-top: 10px;}
    .badge{display:inline-block; padding: 2px 10px; border:1px solid var(--border); border-radius: 999px; font-size: 0.9rem; color: var(--muted);}
    .callout{background: var(--callout); border:1px solid var(--border); border-radius: 12px; padding: 14px 14px; margin: 16px 0;}
    .callout.warn{background: var(--warn); border-color: var(--warnborder);}
    .callout.good{background: var(--good); border-color: var(--goodborder);}
    .callout.bad{background: var(--bad); border-color: var(--badborder);}
    .toc{border:1px solid var(--border); border-radius: 12px; padding: 14px 14px; background: #fafafa;}
    .toc ol{margin: 8px 0 0 18px;}
    .toc li{margin: 6px 0;}
    table{width:100%; border-collapse: collapse; margin: 12px 0;}
    th,td{border:1px solid var(--border); padding: 10px 10px; vertical-align: top;}
    th{background:#fafafa; text-align:left;}
    .small{font-size: 0.95rem; color: var(--muted);}
    .kpi{display:flex; gap:12px; flex-wrap: wrap; margin: 10px 0 0;}
    .kpi .card{flex:1 1 220px; border:1px solid var(--border); border-radius: 12px; padding: 12px 12px; background:#fff;}
    .kpi .label{font-size: 0.9rem; color: var(--muted);}
    .kpi .value{font-size: 1.35rem; font-weight: 700; margin-top: 2px;}
    code, pre{font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono","Courier New", monospace;}
    pre{background: var(--codebg); color: var(--codefg); padding: 14px; border-radius: 12px; overflow:auto; border: 1px solid #111827;}
    .pill{display:inline-block; padding: 2px 8px; border:1px solid var(--border); border-radius: 999px; font-size: 0.85rem; color: var(--muted); margin-left: 6px;}
    .barwrap{border:1px solid var(--border); border-radius: 12px; padding: 14px; background:#fff;}
    .barrow{display:flex; align-items:center; gap:10px; margin: 9px 0;}
    .barlabel{width: 215px; font-size: 0.95rem; color: var(--muted);}
    .bar{height: 14px; border-radius: 999px; background:#e5e7eb; flex: 1;}
    .bar > span{display:block; height: 14px; border-radius: 999px; background: #111827;}
    .barvalue{width: 86px; text-align:right; font-variant-numeric: tabular-nums;}
    .hr{height:1px; background:var(--border); margin: 18px 0;}
    .footnotes{font-size: 0.95rem; color: var(--muted);}
    .ref li{margin: 10px 0;}
    .nowrap{white-space: nowrap;}
    .gh-footer{text-align:center; margin-top: 3rem; padding: 2rem 0; border-top: 1px solid var(--border);}
    .gh-footer a{font-weight: 600; font-size: 1.1rem;}
    .gh-footer .gh-desc{font-size: 0.9rem; color: var(--muted); margin-top: 6px;}
    .gh-footer .gh-desc a{font-weight: normal; font-size: inherit;}
  </style>
</head>

<body>
<main>
  <header>
    <div class="badge">Peer-reviewed revision</div>
    <h1>Memory-Assisted Learning vs Recursive Language Models</h1>
    <p class="meta">
      An empirical study in Tic-Tac-Toe with an LLM learner, a minimax opponent, and a lessons buffer.
    </p>
    <div class="author">Laurent DeSegur</div>
    <p class="small">February 2026 &middot; KarnEvil9 Project &middot; 72 games &middot; 72 lessons (36/phase) &middot; 6 experimental runs</p>

    <div class="callout warn">
      <strong>Terminology update (important):</strong>
      In recent literature, <em>Recursive Language Models (RLMs)</em> refers to an inference paradigm where an LLM treats the prompt as an external environment and can programmatically inspect, decompose, and recursively invoke sub-calls over slices of the input (Zhang, Kraska, Khattab 2025). This report's Phase B is a simplified <em>prompt-recursive lesson injection</em> baseline, not a full implementation of the MIT CSAIL RLM framework.
      <div class="small" style="margin-top:8px;">
        See: <a href="https://arxiv.org/abs/2512.24601">arXiv:2512.24601</a> and the reference list for details.
      </div>
    </div>

    <nav class="toc" aria-label="Table of contents">
      <strong>Contents</strong>
      <ol>
        <li><a href="#abstract">Abstract</a></li>
        <li><a href="#background">Background and related work</a></li>
        <li><a href="#setup">Experimental setup</a></li>
        <li><a href="#results">Results</a></li>
        <li><a href="#analysis">Analysis and interpretation</a></li>
        <li><a href="#economics">Token economics and scaling behavior</a></li>
        <li><a href="#failure-modes">Failure modes</a></li>
        <li><a href="#validity">Threats to validity</a></li>
        <li><a href="#lessons">Lessons learned</a></li>
        <li><a href="#next">What's next</a></li>
        <li><a href="#repro">Reproducibility checklist</a></li>
        <li><a href="#refs">References</a></li>
      </ol>
    </nav>
  </header>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 1. ABSTRACT                                                    -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="abstract">
    <h2>1. Abstract</h2>
    <p>
      This report compares two ways of improving an LLM agent's performance via iterative self-generated learning signals while playing Tic-Tac-Toe against a strong (minimax) opponent with a small blunder rate:
      <strong>(A) Memory-Assisted Learning (MAL)</strong>, where lessons are distilled into deterministic code rules that can override the LLM for specific decision classes, and
      <strong>(B) Prompt-recursive lessons</strong>, where the LLM's own lessons are appended back into its prompt and all decisions remain natural-language mediated.
    </p>

    <div class="kpi">
      <div class="card">
        <div class="label">MAL loss rate</div>
        <div class="value">41.7% &rarr; 0%</div>
        <div class="small">Run 1 to Run 3 (36 games total)</div>
      </div>
      <div class="card">
        <div class="label">Prompt-recursive loss rate</div>
        <div class="value">41.7% &rarr; 25.0%</div>
        <div class="small">Run 1 to Run 3 (36 games total)</div>
      </div>
      <div class="card">
        <div class="label">Dominant failure mode</div>
        <div class="value">Missed blocks</div>
        <div class="small">Two-in-a-row threats not blocked</div>
      </div>
    </div>

    <p>
      Key finding: both approaches quickly produce <em>correct strategic statements</em> (for example, "block two-in-a-row threats"),
      but prompt-only recursion struggles to turn those statements into reliable move-level procedures.
      MAL closes this "knowing-doing gap" by promoting stable insights into deterministic code, yielding monotonic improvement and lower token cost once rules activate.
    </p>

    <div class="callout good">
      <strong>Key distinction:</strong>
      In RL, learning is stored in weights. In prompt-recursive learning, learning is stored in context.
      The model doesn't become better&mdash;its input becomes richer. No weights change at any point in either phase.
    </div>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 2. BACKGROUND                                                  -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="background">
    <h2>2. Background and related work</h2>
    <p>
      This experiment sits at the intersection of three themes in modern LLM systems:
      external memory and retrieval, agentic tool use, and inference-time scaffolding for long context.
      Below are a few relevant research threads to contextualize this study.
    </p>

    <h3>Memory augmentation and retrieval</h3>
    <ul>
      <li>
        <strong>Retrieval-Augmented Generation (RAG):</strong> combines parametric memory with retrieved documents as non-parametric memory for knowledge-intensive tasks.
        <span class="pill">Lewis et al. 2020</span>
      </li>
      <li>
        <strong>kNN-LMs and RETRO:</strong> retrieval-augmented language modeling via nearest-neighbor datastores or large retrieval corpora.
        <span class="pill">Khandelwal et al. 2020</span>
        <span class="pill">Borgeaud et al. 2022</span>
      </li>
      <li>
        <strong>Agent memory hierarchies:</strong> systems that manage memory tiers and decide what to page in/out of context.
        <span class="pill">Packer et al. 2023 (MemGPT)</span>
      </li>
    </ul>

    <h3>Language feedback loops and agentic learning without weight updates</h3>
    <ul>
      <li>
        <strong>Reflexion:</strong> improves agents using verbal reflection stored in an episodic memory buffer, not by updating weights.
        <span class="pill">Shinn et al. 2023</span>
      </li>
      <li>
        <strong>Generative Agents:</strong> structured memory plus reflection to support long-horizon behavior in simulations.
        <span class="pill">Park et al. 2023</span>
      </li>
    </ul>

    <h3>Tool use, reasoning-and-acting, and programmatic scaffolds</h3>
    <ul>
      <li>
        <strong>ReAct:</strong> interleaves reasoning traces with actions that query external environments.
        <span class="pill">Yao et al. 2022</span>
      </li>
      <li>
        <strong>Toolformer:</strong> trains LMs to decide when to call external tools and incorporate results.
        <span class="pill">Schick et al. 2023</span>
      </li>
    </ul>

    <h3>Long-context limitations and newer RLM work</h3>
    <ul>
      <li>
        <strong>Context degradation ("context rot"):</strong> empirical evidence that performance degrades with increasing input length in non-uniform ways, even when the context fits.
        <span class="pill">Chroma Research 2025</span>
      </li>
      <li>
        <strong>RULER:</strong> a benchmark that extends needle-in-a-haystack style tests into multi-hop and aggregation tasks, showing large drops as context grows.
        <span class="pill">Hsieh et al. 2024</span>
      </li>
      <li>
        <strong>Interactive reading:</strong> treating the model as an agent that decides how to read long contexts via iterative prompting (MemWalker).
        <span class="pill">Chen et al. 2023</span>
      </li>
      <li>
        <strong>Recursive Language Models (RLMs):</strong> an inference strategy that treats long prompts as an external environment and allows programmatic decomposition and recursive sub-calls.
        <span class="pill">Zhang, Kraska, Khattab 2025</span>
      </li>
    </ul>

    <div class="callout">
      <strong>Where this study fits:</strong>
      MAL is best understood as a light-weight agent architecture: the LLM creates and updates a natural-language memory, but deterministic code executes critical subroutines.
      Phase B is closer to "reflection-in-context" (prompt-recursive learning) than to the 2025 MIT CSAIL RLM definition.
    </div>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 3. EXPERIMENTAL SETUP                                          -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="setup">
    <h2>3. Experimental setup</h2>

    <h3>Task</h3>
    <p>
      Tic-Tac-Toe is used as a controlled environment where optimal play is known and failure modes are easy to classify.
      The learner always plays <strong>O</strong> and <strong>moves first</strong>&mdash;a deliberate design choice
      that gives Claude the theoretical first-mover advantage. The opponent plays X (second) using a <strong>deterministic
      minimax</strong> algorithm with a <strong>10% random blunder rate</strong>: on any given move there is a 1-in-10
      chance the expert picks a random legal square instead of the optimal one. This is the <em>only</em> weakness in the
      opponent&mdash;a narrow, stochastic window that the learner must learn to exploit.
    </p>

    <p>
      The experimental setup thus asks a precise question: <strong>given first-move advantage and a near-perfect opponent
      with a known 10% weak spot, can an LLM learn to reliably exploit that weakness through accumulated experience?</strong>
    </p>

    <h3>Agent testbed</h3>
    <p>
      The system is a three-node KarnEvil9 swarm mesh that runs repeated games and extracts lessons after each game:
    </p>
    <ul>
      <li><strong>Learner agent (O):</strong> chooses moves; receives accumulated lesson text. Moves first (advantage).</li>
      <li><strong>Opponent agent (X):</strong> deterministic minimax policy with 10% random blunder. Moves second.</li>
      <li><strong>Referee node:</strong> coordinates games, delegates moves, extracts lessons from transcripts.</li>
    </ul>

    <h3>Phases</h3>

    <h4>Phase A: Memory-Assisted Learning (MAL)</h4>
    <p>
      The MAL phase uses an LLM for general reasoning and lesson generation, but introduces
      <strong>programmatic rules</strong> that can bypass the model when certain conditions are met
      (for example, immediate win opportunities or mandatory blocks).
      The intent is to convert reliable high-level insights into deterministic execution.
    </p>

    <h4>Phase B: Prompt-recursive lessons (simplified baseline)</h4>
    <p>
      The Phase B baseline removes the programmatic rule layer. Every learner move is produced by the LLM,
      guided only by the accumulated lesson text in its prompt.
      The original experiment used a stronger model in this phase (Claude Sonnet 4.5) to give the prompt-only approach the best chance.
    </p>

    <h3>Controls</h3>
    <ul>
      <li>Memory wiped between phases (Phase B starts with 0 lessons).</li>
      <li>Same opponent configuration (deterministic minimax + 10% random blunder).</li>
      <li>Same lesson extraction prompt.</li>
      <li>Learner always plays O and moves first.</li>
      <li>12 games per run, lessons persist across runs within each phase.</li>
    </ul>

    <h3>Evaluation metrics</h3>
    <ul>
      <li><strong>Win / loss / draw counts</strong> per run.</li>
      <li><strong>Loss rate</strong> (losses / 12 games) per run.</li>
      <li><strong>Failure mode labeling</strong> for each loss (missed block, fork trap, etc.).</li>
    </ul>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 4. RESULTS                                                     -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="results">
    <h2>4. Results</h2>

    <h3>Phase A: Memory-Assisted Learning (MAL)</h3>
    <table>
      <thead>
        <tr>
          <th>Run</th>
          <th class="nowrap">Lessons in</th>
          <th>Wins</th>
          <th>Losses</th>
          <th>Draws</th>
          <th class="nowrap">Loss rate</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>MAL-1 (fresh)</td><td>0</td><td>2</td><td>5</td><td>5</td><td>41.7%</td>
        </tr>
        <tr>
          <td>MAL-2</td><td>12</td><td>3</td><td>1</td><td>8</td><td>8.3%</td>
        </tr>
        <tr>
          <td>MAL-3</td><td>24</td><td>4</td><td>0</td><td>8</td><td>0%</td>
        </tr>
      </tbody>
    </table>

    <div class="barwrap" aria-label="MAL loss rate by run">
      <strong>MAL loss rate by run</strong>
      <div class="barrow">
        <div class="barlabel">MAL-1 (0 lessons)</div>
        <div class="bar"><span style="width:41.7%"></span></div>
        <div class="barvalue">41.7%</div>
      </div>
      <div class="barrow">
        <div class="barlabel">MAL-2 (12 lessons)</div>
        <div class="bar"><span style="width:8.3%"></span></div>
        <div class="barvalue">8.3%</div>
      </div>
      <div class="barrow">
        <div class="barlabel">MAL-3 (24 lessons)</div>
        <div class="bar"><span style="width:0%"></span></div>
        <div class="barvalue">0%</div>
      </div>
      <div class="small" style="margin-top:8px;">
        MAL reaches 0 losses by run 3. This coincides with activation of deterministic block logic that prevents missed two-in-a-row defenses.
      </div>
    </div>

    <h3>Phase B: Prompt-recursive lessons baseline</h3>
    <table>
      <thead>
        <tr>
          <th>Run</th>
          <th class="nowrap">Lessons in</th>
          <th>Wins</th>
          <th>Losses</th>
          <th>Draws</th>
          <th class="nowrap">Loss rate</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>B-1 (fresh)</td><td>0</td><td>1</td><td>5</td><td>6</td><td>41.7%</td>
        </tr>
        <tr>
          <td>B-2</td><td>12</td><td>2</td><td>6</td><td>4</td><td>50.0%</td>
        </tr>
        <tr>
          <td>B-3</td><td>24</td><td>1</td><td>3</td><td>8</td><td>25.0%</td>
        </tr>
      </tbody>
    </table>

    <div class="barwrap" aria-label="Prompt-recursive loss rate by run">
      <strong>Prompt-recursive loss rate by run</strong>
      <div class="barrow">
        <div class="barlabel">B-1 (0 lessons)</div>
        <div class="bar"><span style="width:41.7%"></span></div>
        <div class="barvalue">41.7%</div>
      </div>
      <div class="barrow">
        <div class="barlabel">B-2 (12 lessons)</div>
        <div class="bar"><span style="width:50.0%"></span></div>
        <div class="barvalue">50.0%</div>
      </div>
      <div class="barrow">
        <div class="barlabel">B-3 (24 lessons)</div>
        <div class="bar"><span style="width:25.0%"></span></div>
        <div class="barvalue">25.0%</div>
      </div>
      <div class="small" style="margin-top:8px;">
        Note the non-monotonic trajectory: more lessons can help, but can also add noise or conflicting guidance.
      </div>
    </div>

    <h3>Head-to-head comparison</h3>
    <table>
      <thead>
        <tr>
          <th>Run</th>
          <th class="nowrap">MAL loss rate</th>
          <th class="nowrap">Prompt-recursive loss rate</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>Run 1 (fresh)</td><td>41.7%</td><td>41.7%</td></tr>
        <tr><td>Run 2 (12 lessons)</td><td>8.3%</td><td>50.0%</td></tr>
        <tr><td>Run 3 (24 lessons)</td><td>0%</td><td>25.0%</td></tr>
      </tbody>
    </table>

    <h3>Failure mode breakdown</h3>
    <table>
      <thead>
        <tr>
          <th>Failure mode (losses only)</th>
          <th>MAL occurrences</th>
          <th>Prompt-recursive occurrences</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Failed to block a two-in-a-row threat</td>
          <td>4 (67%)</td>
          <td>9 (64%)</td>
        </tr>
        <tr>
          <td>Fell into a fork trap</td>
          <td>1 (17%)</td>
          <td>3 (21%)</td>
        </tr>
        <tr>
          <td>Poor positional play (edges over corners)</td>
          <td>1 (17%)</td>
          <td>2 (14%)</td>
        </tr>
      </tbody>
    </table>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 5. ANALYSIS                                                    -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="analysis">
    <h2>5. Analysis and interpretation</h2>

    <h3>The knowing-doing gap</h3>
    <p>
      After only a few games, both systems produced lessons that describe correct strategy:
      "block two-in-a-row threats immediately", "prioritize corners over edges", "avoid fork setups".
      The surprise is that the prompt-recursive baseline still repeatedly failed at the simplest procedural task:
      reliably <em>detecting</em> and <em>blocking</em> imminent threats on the board.
    </p>

    <p>
      A natural-language lesson like "always block when the opponent has two in a row" is correct but underspecified as an algorithm.
      It does not force an explicit scan over all eight winning lines or a deterministic mapping from board state to block action.
      In other words, the lesson is <em>strategic</em> but not <em>procedural</em>.
      MAL reduces this gap by compiling the strategy into deterministic control logic.
    </p>

    <h3>Convergence characteristics</h3>
    <p>
      MAL improves monotonically (5 &rarr; 1 &rarr; 0 losses), consistent with the idea that deterministic rules do not regress once enabled.
      The prompt-recursive baseline is non-monotonic (5 &rarr; 6 &rarr; 3 losses): adding more lesson text can introduce conflict,
      redundancy, or prompt drift, which is consistent with broader observations that longer contexts do not guarantee better performance.
    </p>

    <div class="callout">
      <strong>Connection to long-context research:</strong>
      While this experiment is small, the non-monotonic behavior aligns with findings from long-context evaluation work showing
      that performance often degrades as context grows, and that naive "just add more context" approaches are brittle.
      See Chroma's Context Rot report and the RULER benchmark in the references.
    </div>

    <h3>Purity vs practicality</h3>
    <p>
      The prompt-recursive baseline is attractive because it is simple: no special tooling and no hand-written heuristics.
      However, in environments where correctness depends on exact symbolic checks (like threat detection),
      handing control back to deterministic code can be a large win in both reliability and compute.
    </p>

    <p>
      The prompt-recursive approach used Claude Sonnet 4.5 (a significantly more capable model than Haiku 4.5) yet still
      underperformed MAL with Haiku. This suggests that the bottleneck isn't model capability&mdash;it's the
      <strong>representation gap</strong> between natural language lessons and procedural execution. A weaker model
      with code-level scaffolding outperforms a stronger model with only natural language self-guidance.
    </p>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 6. TOKEN ECONOMICS                                             -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="economics">
    <h2>6. Token economics and scaling behavior</h2>

    <h3>Per-move token growth</h3>
    <p>
      Both approaches inject lessons into the learner prompt, so input tokens grow approximately linearly with the number of lessons:
    </p>

    <table>
      <thead>
        <tr>
          <th>Lessons in memory</th>
          <th>Estimated input tokens per move</th>
          <th>Where observed</th>
        </tr>
      </thead>
      <tbody>
        <tr><td>0</td><td>~300</td><td>Run 1, game 1</td></tr>
        <tr><td>12</td><td>~660</td><td>Run 2, game 1</td></tr>
        <tr><td>24</td><td>~1,020</td><td>Run 3, game 1</td></tr>
        <tr><td>36</td><td>~1,380</td><td>Run 3, game 12</td></tr>
      </tbody>
    </table>

    <h3>Decomposing the cost advantage</h3>
    <p>
      Once deterministic rules are active, MAL reduces the number of LLM calls per game by bypassing the model
      for forced tactical decisions (immediate wins and mandatory blocks):
    </p>

    <table>
      <thead>
        <tr>
          <th></th>
          <th>MAL (Run 3)</th>
          <th>Prompt-recursive (Run 3)</th>
          <th>Prompt-recursive (same model)</th>
        </tr>
      </thead>
      <tbody>
        <tr><td><strong>LLM calls per game</strong></td><td>~3.5</td><td>~6</td><td>~6</td></tr>
        <tr><td><strong>Total tokens per game</strong></td><td>~3,100</td><td>~5,800</td><td>~5,800</td></tr>
        <tr><td><strong>Estimated per run (12 games)</strong></td><td>~37,000</td><td>~70,000</td><td>~70,000</td></tr>
        <tr><td><strong>Model</strong></td><td>Haiku 4.5</td><td>Sonnet 4.5</td><td>Haiku 4.5</td></tr>
        <tr><td><strong>Estimated cost per run</strong></td><td>~$0.03</td><td>~$0.25</td><td>~$0.06</td></tr>
      </tbody>
    </table>

    <p>
      The headline comparison&mdash;MAL at ~$0.03 vs prompt-recursive at ~$0.25&mdash;is ~8&times;, but this conflates two independent variables:
    </p>

    <ol>
      <li><strong>Architectural advantage (~2&times;):</strong> MAL's programmatic rules bypass Claude for
      ~50% of moves. This savings is intrinsic to the architecture&mdash;code execution is always cheaper
      than LLM inference, regardless of which model you use.</li>
      <li><strong>Model pricing advantage (~4&times;):</strong> MAL used Haiku ($0.80/M input) while Phase B
      used Sonnet ($3/M input). This was a deliberate experimental choice&mdash;MAL's scaffolding
      compensates for a weaker model&mdash;but it's a design decision, not an inherent property of MAL.</li>
    </ol>

    <p>
      The third column shows the fair apples-to-apples comparison: prompt-recursive on the same Haiku model would cost
      ~$0.06 per run, making MAL only ~2&times; cheaper. The architectural token savings is real but modest.
      The larger implication is that MAL <strong>enables the use of cheaper models</strong> by offloading
      critical decisions to code, while prompt-recursive learning demands a more capable (and more expensive) model because
      every decision flows through the LLM.
    </p>

    <h3>Scaling: neither has infinite context</h3>

    <p>
      A natural question is whether the prompt-recursive approach offers a scaling advantage through "infinite context."
      This is a misconception. <strong>Both approaches are bounded by the context window</strong>, and MAL actually scales better:
    </p>

    <table>
      <thead>
        <tr>
          <th>Scaling property</th>
          <th>MAL</th>
          <th>Prompt-recursive</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><strong>Knowledge in context</strong></td>
          <td>Only unpromoted lessons</td>
          <td>All lessons (entire history)</td>
        </tr>
        <tr>
          <td><strong>Knowledge in code</strong></td>
          <td>Promoted rules (zero tokens)</td>
          <td>None</td>
        </tr>
        <tr>
          <td><strong>Context pressure at scale</strong></td>
          <td>Low (lessons graduate to code)</td>
          <td>High (all lessons stay in prompt)</td>
        </tr>
        <tr>
          <td><strong>Cost trajectory</strong></td>
          <td>Flattens as rules absorb patterns</td>
          <td>Grows linearly with experience</td>
        </tr>
      </tbody>
    </table>

    <p>
      In MAL, high-confidence lessons are <strong>promoted to deterministic code</strong>, removing them
      from the context window entirely. MAL's context pressure <em>decreases</em> as the system matures,
      while prompt-recursive context pressure only grows.
    </p>

    <div class="callout warn">
      <strong>Note:</strong> cost values are estimates that depend on provider pricing, tokenization details, and exact prompts.
      Use them as order-of-magnitude guidance, not a guaranteed bill.
    </div>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 7. FAILURE MODES                                               -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="failure-modes">
    <h2>7. Failure modes</h2>

    <h3>Training signals for recursive control</h3>

    <p>
      Our experiment fixed the recursion structure externally: 12 games per run, 3 runs per phase, one lesson
      per game. A full recursive system would need the model to control its own recursion&mdash;deciding <em>when</em>
      to recurse deeper, <em>when</em> to consolidate, and <em>when</em> to stop. The data suggests three
      training signals that would be needed:
    </p>

    <p>
      <strong>Lesson similarity as a halt signal.</strong> By run 3, over 70% of lessons were restatements of
      "take opposite corners" and "block two-in-a-row." A system measuring semantic similarity between new
      and existing lessons could detect this saturation. Signal: <em>if the last K lessons have cosine similarity
      &gt; 0.85 with existing lessons, stop recursing and synthesize.</em>
    </p>

    <p>
      <strong>Performance plateau as a depth signal.</strong> Phase B run 2 performed <em>worse</em> than run 1
      despite twice the lessons. A system monitoring its own win/loss rate across iterations could
      detect this regression. Signal: <em>if loss rate hasn't decreased over N iterations, change the recursion
      strategy rather than continuing the current one.</em>
    </p>

    <p>
      <strong>Contradiction detection as a consolidation trigger.</strong> Our data shows directly contradictory
      lessons accumulating: game 5 says "prioritize offense over defense," game 6 says "block before pursuing
      threats." Signal: <em>if lessons contain opposing directives for the same game state, recurse into a
      consolidation pass before continuing play.</em>
    </p>

    <p>
      These signals point toward a <strong>meta-recursive architecture</strong>: the outer loop generates
      experience, an inner loop monitors the quality of that experience, and a control layer decides whether
      to continue, consolidate, or terminate. The performance regression in Phase B run 2 is exactly what
      happens without it.
    </p>

    <h3>Looping hallucinations</h3>

    <p>
      The self-referential feedback loop creates a specific failure mode that traditional RL systems
      don't face: <strong>looping hallucinations</strong>, where the model's own incorrect outputs compound
      through iterations, creating a degenerative spiral.
    </p>

    <p>
      We observed a mild form in our data. The learner generated contradictory lessons across games 5-6:
      one advocating offense, the other defense. In subsequent games the model oscillated between them,
      unable to determine which applies to the current board state because the lessons lack
      conditional specificity to disambiguate.
    </p>

    <div class="callout bad">
      <strong>Degenerative loop:</strong> Model produces incorrect analysis &rarr; incorrect analysis
      becomes a lesson &rarr; lesson biases future reasoning &rarr; biased reasoning produces more
      incorrect analysis &rarr; each iteration reinforces the error. Unlike RL, where gradient updates
      can correct course through reward signals, prompt-recursive learning has <strong>no error-correction
      mechanism</strong>. A bad lesson persists in the context window indefinitely.
    </div>

    <p>
      In our experiment, this was bounded by tic-tac-toe's simplicity. In open-ended domains (code generation,
      strategic planning, research synthesis), a looping hallucination could be catastrophic: the model
      confidently generates an incorrect pattern, encodes it as a lesson, and applies it to every subsequent
      task, each application generating a new lesson that further reinforces the mistake.
    </p>

    <p><strong>Mitigations:</strong></p>
    <ul>
      <li><strong>External validation:</strong> ground-truth checks on lesson accuracy before they enter the store.</li>
      <li><strong>Lesson decay:</strong> reduce weight of older lessons, allowing the system to "forget" early mistakes.</li>
      <li><strong>Adversarial self-critique:</strong> ask the model to argue against each new lesson before accepting it.</li>
      <li><strong>Promotion to code (MAL's approach):</strong> code is verifiable in ways that natural language lessons are not.</li>
    </ul>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 8. THREATS TO VALIDITY                                         -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="validity">
    <h2>8. Threats to validity</h2>
    <ul>
      <li>
        <strong>Model mismatch across phases:</strong> the prompt-recursive baseline used a stronger model (Sonnet) than MAL (Haiku).
        This makes the result conservative in MAL's favor on reliability, but it complicates attributing differences solely to architecture.
      </li>
      <li>
        <strong>Small sample size:</strong> 36 games per phase is enough to observe large effect sizes (like 0 vs non-zero losses),
        but not enough to precisely estimate variance or small differences.
      </li>
      <li>
        <strong>Non-independence:</strong> lessons accumulate across runs within a phase, so runs are not independent trials.
      </li>
      <li>
        <strong>Opponent stochasticity:</strong> a 10% blunder rate adds variance; some wins/losses may be opponent noise rather than learner improvement.
      </li>
      <li>
        <strong>Prior knowledge:</strong> LLMs likely have pretraining exposure to Tic-Tac-Toe optimal strategy.
        Measured improvements are about <em>execution consistency</em> given a prompt and scaffolding, not about learning the game from scratch.
      </li>
    </ul>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 9. LESSONS LEARNED                                             -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="lessons">
    <h2>9. Lessons learned</h2>

    <p>
      This toy domain is small enough to make failure modes legible, and that is the point.
      The experiments suggest a few practical lessons about "learning without weight updates" in LLM agents.
    </p>

    <ul>
      <li>
        <strong>Natural-language advice is not a procedure.</strong>
        The agent can often verbalize correct strategy while still failing to execute the corresponding scan-and-act routine.
        Reliability improves when the strategy is compiled into an explicit algorithm.
      </li>
      <li>
        <strong>Memory helps until it becomes clutter.</strong>
        Accumulating lessons can improve behavior, but it can also introduce redundancy, conflict, and prompt drift.
        Without deduplication and consolidation, longer lesson buffers can be non-monotonic in performance.
      </li>
      <li>
        <strong>Deterministic promotion yields monotonicity.</strong>
        Once a rule is expressed as code, it does not "forget" or regress, and it lowers variance introduced by sampling.
      </li>
      <li>
        <strong>Benchmarks must separate policy improvement from opponent randomness.</strong>
        A blunder-rate opponent is a reasonable way to avoid draw-only play, but it injects variance.
        Reporting random seeds, realized blunders, and confidence intervals matters even in small domains.
      </li>
      <li>
        <strong>Terminology needs to match the current literature.</strong>
        "Recursive Language Models" has a specific meaning in recent long-context work.
        If the system is a lesson-injection loop rather than a full prompt-as-environment recursive decomposition framework, naming it explicitly prevents confusion.
      </li>
    </ul>

    <div class="callout good">
      <strong>Design pattern:</strong>
      Use the LLM to explore, hypothesize, and summarize.
      Use deterministic code to execute the parts that demand perfect precision or exhaustive checks.
      Treat promotion-to-code as a reliability and cost optimization, not as a retreat from learning.
    </div>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 10. WHAT'S NEXT                                                -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="next">
    <h2>10. What's next</h2>

    <p>
      The current results are useful as a mechanistic demonstration, but they are not yet strong evidence about general learning dynamics.
      The most valuable follow-ups fall into three buckets: tightening evaluation, strengthening the memory loop, and testing generality.
    </p>

    <h3>Tighten the evaluation</h3>
    <ol>
      <li>
        <strong>Remove the model confound:</strong> run both conditions on the same base model (and ideally on at least two models) to isolate architecture from capability.
      </li>
      <li>
        <strong>Add a true no-learning baseline:</strong> same prompts and model, but with lessons disabled and no deterministic rules.
      </li>
      <li>
        <strong>Increase sample size and control randomness:</strong> run more games per condition, fix opponent seeds, and report confidence intervals for loss rates and key failure modes.
      </li>
      <li>
        <strong>Measure tokens and latency:</strong> replace estimated token counts with measured input-output token logs per call, per game, and per run.
      </li>
    </ol>

    <h3>Strengthen the learning loop</h3>
    <ol>
      <li>
        <strong>Lesson consolidation:</strong> deduplicate and merge lessons into a small canonical set, with explicit conflict detection and versioning.
      </li>
      <li>
        <strong>Verification gates:</strong> before accepting a lesson, test it against a verifier or simulator.
        For Tic-Tac-Toe this can be exact: validate that a proposed rule never increases loss probability against minimax.
      </li>
      <li>
        <strong>Structured lessons:</strong> experiment with JSON schemas or checklists (for example, "scan all 8 lines, block if any has 2 opponent marks and 1 empty").
        This tests whether failures are due to language ambiguity or due to attention and execution.
      </li>
      <li>
        <strong>Full RLM-style scaffolding:</strong> implement a prompt-as-environment REPL workflow and recursive sub-calls to align with modern RLM definitions in the literature.
      </li>
    </ol>

    <h3>Test generality beyond Tic-Tac-Toe</h3>
    <ol>
      <li>
        <strong>Harder games:</strong> small board games with larger state spaces and more tactical patterns (for example, Connect Four) will stress both search and memory.
      </li>
      <li>
        <strong>Long-context tasks:</strong> apply the same MAL vs prompt-recursive baseline to long-document QA, codebase navigation, or deep-research benchmarks.
      </li>
      <li>
        <strong>Agentic workflows:</strong> evaluate whether promotion-to-code improves stability in multi-step tool-use tasks, where verbal plans often fail at execution details.
      </li>
    </ol>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 11. REPRODUCIBILITY CHECKLIST                                  -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="repro">
    <h2>11. Reproducibility checklist</h2>

    <div class="callout">
      <strong>Code location:</strong> <a href="https://github.com/oldeucryptoboi/KarnEvil9">github.com/oldeucryptoboi/KarnEvil9</a>
      <br />
      <span class="small">Experiment driver: <a href="https://github.com/oldeucryptoboi/KarnEvil9/blob/master/scripts/tic-tac-toe-swarm.ts"><code>scripts/tic-tac-toe-swarm.ts</code></a></span>
    </div>

    <ul>
      <li><strong>Models:</strong> Claude Haiku 4.5 (Phase A), Claude Sonnet 4.5 (Phase B).</li>
      <li><strong>Opponent policy:</strong> deterministic minimax + 10% random blunder rate.</li>
      <li><strong>Runs:</strong> 3 runs per phase, 12 games per run.</li>
      <li><strong>Lesson persistence:</strong> within-phase accumulation (0 &rarr; 12 &rarr; 24 lessons).</li>
      <li><strong>Key prompts:</strong> system prompt for the learner, and the post-game lesson extraction prompt.</li>
      <li><strong>Random seeds:</strong> record and fix seeds for opponent blunders and any sampling (if applicable).</li>
      <li><strong>Logging:</strong> store full game transcripts and lessons per game.</li>
    </ul>

    <p class="small">
      If you are publishing results, consider adding: exact model versions, temperature/top-p settings, max tokens, and a reproducible environment lockfile.
    </p>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- 12. REFERENCES                                                 -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <section id="refs">
    <h2>12. References</h2>
    <ol class="ref">
      <li>
        Zhang, A. L., Kraska, T., Khattab, O. (2025). <em>Recursive Language Models</em>. arXiv:2512.24601.
        <a href="https://arxiv.org/abs/2512.24601">arXiv</a>,
        <a href="https://arxiv.org/pdf/2512.24601">PDF</a>,
        <a href="https://github.com/alexzhang13/rlm">code</a>.
      </li>
      <li>
        Lewis, P. et al. (2020). <em>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em>. arXiv:2005.11401.
        <a href="https://arxiv.org/abs/2005.11401">arXiv</a>.
      </li>
      <li>
        Khandelwal, U. et al. (2020). <em>Generalization through Memorization: Nearest Neighbor Language Models</em>. ICLR 2020. arXiv:1911.00172.
        <a href="https://arxiv.org/abs/1911.00172">arXiv</a>.
      </li>
      <li>
        Borgeaud, S. et al. (2022). <em>Improving Language Models by Retrieving from Trillions of Tokens</em>. ICML 2022. arXiv:2112.04426.
        <a href="https://arxiv.org/abs/2112.04426">arXiv</a>.
      </li>
      <li>
        Packer, C. et al. (2023). <em>MemGPT: Towards LLMs as Operating Systems</em>. arXiv:2310.08560.
        <a href="https://arxiv.org/abs/2310.08560">arXiv</a>.
      </li>
      <li>
        Shinn, N. et al. (2023). <em>Reflexion: Language Agents with Verbal Reinforcement Learning</em>. arXiv:2303.11366.
        <a href="https://arxiv.org/abs/2303.11366">arXiv</a>.
      </li>
      <li>
        Park, J. S. et al. (2023). <em>Generative Agents: Interactive Simulacra of Human Behavior</em>. arXiv:2304.03442.
        <a href="https://arxiv.org/abs/2304.03442">arXiv</a>.
      </li>
      <li>
        Yao, S. et al. (2022). <em>ReAct: Synergizing Reasoning and Acting in Language Models</em>. arXiv:2210.03629.
        <a href="https://arxiv.org/abs/2210.03629">arXiv</a>.
      </li>
      <li>
        Schick, T. et al. (2023). <em>Toolformer: Language Models Can Teach Themselves to Use Tools</em>. arXiv:2302.04761.
        <a href="https://arxiv.org/abs/2302.04761">arXiv</a>.
      </li>
      <li>
        Chroma Research (2025). <em>Context Rot: How Increasing Input Tokens Impacts LLM Performance</em>.
        <a href="https://research.trychroma.com/context-rot">report</a>.
      </li>
      <li>
        Hsieh, C.-P. et al. (2024). <em>RULER: What's the Real Context Size of Your Long-Context Language Models?</em> arXiv:2404.06654.
        <a href="https://arxiv.org/abs/2404.06654">arXiv</a>.
      </li>
      <li>
        Chen, H. et al. (2023). <em>Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading</em>. arXiv:2310.05029.
        <a href="https://arxiv.org/abs/2310.05029">arXiv</a>.
      </li>
      <li>
        Chen, Z. et al. (2025). <em>BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent</em>. arXiv:2508.06600.
        <a href="https://arxiv.org/abs/2508.06600">arXiv</a>.
      </li>
    </ol>
  </section>

  <!-- ═══════════════════════════════════════════════════════════════ -->
  <!-- GITHUB FOOTER                                                  -->
  <!-- ═══════════════════════════════════════════════════════════════ -->
  <div class="gh-footer">
    <a href="https://github.com/oldeucryptoboi/KarnEvil9" target="_blank" rel="noopener">
      github.com/oldeucryptoboi/KarnEvil9
    </a>
    <div class="gh-desc">
      Full source code, swarm package, demo scripts, and this paper &mdash;
      <a href="https://github.com/oldeucryptoboi/KarnEvil9/blob/master/scripts/tic-tac-toe-swarm.ts" target="_blank" rel="noopener">tic-tac-toe experiment</a> &middot;
      <a href="https://github.com/oldeucryptoboi/KarnEvil9/tree/master/packages/swarm" target="_blank" rel="noopener">swarm package</a> &middot;
      <a href="https://github.com/oldeucryptoboi/KarnEvil9/tree/master/packages/memory" target="_blank" rel="noopener">memory package</a>
    </div>
  </div>

</main>
</body>
</html>
